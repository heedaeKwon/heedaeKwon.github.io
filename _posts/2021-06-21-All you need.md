# *Attention is All You Need*

---

### Introduction

###### 이전 연구들은  RNN모델로 리커런트한 작업으로 수행을 하였습니다.
###### 이것은 한 단어를 넣는 방식으로 시퀀스에 포함되어있는 순서정보를 정렬하고
###### 반복적으로 넣어서 히든스테이트 값을 갱신시키는 방식이었습니다.
###### 그런 경우 시퀀스의 길이토큰 개수만큼 뉴어럴 네트워크에 넣어야 하기떄문에
###### 병렬적 처리가 불가능하여 문장의 길이가길어지면 그 길이만큼 능력을 수행해야하는 
###### 문제가 있습니다. 그래서 어텐션메커니즘과 같이 사용하여서 보다 효율적으로 
###### 출력단어를 만들 수 있지만 여전히 리커런트 특성으로인한 시퀀스 길이의 제약이 존재하였습니다.

###### 그래서 이 논문은 리커런스한 특성자체를 제거아고 어텐션 메카니즘만을 활용하여 한번의 행렬곱으로
###### 위치정보를 한번에 처리하였고 그렇기에 병렬처리가 가능해져서 기존의 문제점이였던
###### 시퀀스 길이에 대한 제약이 사라져서 좋은 성능이 나왔습니다.

---

### Model Architecture


