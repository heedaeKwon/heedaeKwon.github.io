# cs231n 6강

1. Parameter Updates
     - SGD
     - Momentum update
     - Nesterov Momentum update
     - AdaGrad update
     - RMSProp update
     - Adam update
2. Ensembles
3. dropout

### Parameter Updates

#### SGD

###### 그림으로 예시를 들어보겠다. 

![logo](https://user-images.githubusercontent.com/68374734/107596248-7b837000-6c5a-11eb-8c1c-c5c19f94df0a.PNG)
###### SGD방식으로 해피포인트까지 갈려고할 때를 보면 간격이 좁은 수직부분은 빠르고 수평부분은 느리다. 그래서 벡터로 표현하면 그림과 같다.
###### 그렇게 되면 한번 가고 조정하고 또 조정하고 하는 식으로 된다. 그래서 진행이 다음 사진과 같을 것이다.

![logo](https://user-images.githubusercontent.com/68374734/107595932-8be71b00-6c59-11eb-9d7a-17df4f3f0ae3.PNG)
###### 이렇게 되어서 처리속도가 느리다.

#### Momentum update
###### Momentum 방식은 말 그대로 Gradient Descent를 통해 이동하는 과정에 일종의 ‘관성’을 주는 것이다. 식은 다음과같다

![logd](https://user-images.githubusercontent.com/68374734/107595939-8db0de80-6c59-11eb-9225-c7ea940afdf2.PNG)
###### 요기서 mu는 momentum을 얼마나 줄것인지에 대한 것이고 값은 보통 0.9값을 사용한다.
###### 편히 생각을 하게되면 먼저 방향에 속도를 줘서 발사를하고 점점 속도를 줄여서 목적부분을 찾는 방식입니다.

![logo](https://user-images.githubusercontent.com/68374734/107597266-7ffd5800-6c5d-11eb-9a79-19d48807be65.PNG)

#### Nesterove Momentum update

###### Nesterove는 전의 Momentum과 비슷하지만 차이가있습니다
###### Momentum 방식에서는 이동 벡터 vt 를 계산할 때 현재 위치에서의 gradient와 momentum step을 독립적으로 계산하고 합칩니다.
###### 반면, NAG에서는 momentum step을 먼저 고려하여, momentum step을 먼저 이동했다고 생각한 후 그 자리에서의 gradient를 구해서 gradient step을 이동합니다.
###### 다음 사진과 같습니다.

![logo](https://user-images.githubusercontent.com/68374734/107595938-8d184800-6c59-11eb-922c-c22727bce550.PNG)

#### AdaGrad update

![logo](https://user-images.githubusercontent.com/68374734/107595933-8be71b00-6c59-11eb-8c8b-bb50212c413a.PNG)
###### Adagrad에서 cache는 상태가 변할 떄마다 기울기의 제곱 값이 더해지는 것이다. 그리고 그값으로 나누는데 요기서 1e-7은 0으로 나누는 문제를 방지하기 위해 설정한 것입니다.
###### 이렇게 되니까 Gradient는 cache값과 비례해집니다. 수직층에서는 gradient가 크니 cache값이 커저 분모가 커저가지고 x의 업데이트 속도가 낮아지고
###### 수평측은 gradient가 작아지니 cache 값이 작아져 분모가 작아져 x의 업데이트 속도가 빨라집니다
###### 그러나 adagrad의 문제점은 cache값이 지속적으로 building up 이되어서 이게 깊어지면은 분모에 cache값이 계속 커져 목표를 달성하지 못했는데
###### 0값으로 수렴에 학습이 종료되는 현상이 생깁니다.

#### RMSProp update

![logo](https://user-images.githubusercontent.com/68374734/107595930-8ab5ee00-6c59-11eb-9ace-bf1d7ec7bc13.PNG)
###### 그것을 막고자 한게 RMSProp입니다.
###### 요기서는decay rate을 도입해서 cache값을 서서히 줄이개하는 효과를 내어 adagrad장점을 유지하고 단점을 보완하는 방식입니다.

#### Adam update

![logo](https://user-images.githubusercontent.com/68374734/107595935-8c7fb180-6c59-11eb-9e73-a6c55cbc0f8f.PNG)
####### Adam update RMSProp과 Momentum을 합한 형태입니다. 

### Ensemble

###### 단일모델을 학습시키는 대신에 복수개의 동일 모델을 학습시킵니다.
###### 이들의 결과를 평균을 내는데 이것이 평균 성능보다 2%향상을 시킵니다
###### 대신 트레이닝시에 여러개의 모델을 관리해야하는 이슈와 테스트시에 많은 개수로 인해 linear하게 시간이 증가
###### 보완하는 방법으로는 단일 모델을 epoch들 간에 값들을 ensemble을 하면됩니다. 

### dropout

###### 원래는 보통은 fully connect 형태인데 dropout은 일부 노드를 0으로 닫는 형태입니다. 다음 사진과 같습니다.

![logo](https://user-images.githubusercontent.com/68374734/107595936-8d184800-6c59-11eb-8aad-0c05189a561d.PNG)
###### dropout이 좋은 이유는 여러개의 모델의 parameter값들을 Ensemble하는 효과를 받을 수 있습니다.
