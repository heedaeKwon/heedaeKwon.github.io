# cs231n 6강

1. Parameter Updates
     - SGD
     - Momentum update
     - Nesterov Momentum update
     - AdaGrad update
     - RMSProp update
     - Adam update
2. Ensembles
3. dropout

### Parameter Updates

#### SGD

###### 그림으로 예시를 들어보겠다. 

![logo](https://user-images.githubusercontent.com/68374734/107596248-7b837000-6c5a-11eb-8c1c-c5c19f94df0a.PNG)
###### SGD방식으로 해피포인트까지 갈려고할 때를 보면 간격이 좁은 수직부분은 빠르고 수평부분은 느리다. 그래서 벡터로 표현하면 그림과 같다.
###### 그렇게 되면 한번 가고 조정하고 또 조정하고 하는 식으로 된다. 그래서 진행이 다음 사진과 같을 것이다.

![logo](https://user-images.githubusercontent.com/68374734/107595932-8be71b00-6c59-11eb-9d7a-17df4f3f0ae3.PNG)
###### 이렇게 되어서 처리속도가 느리다.

#### Momentum update
###### Momentum 방식은 말 그대로 Gradient Descent를 통해 이동하는 과정에 일종의 ‘관성’을 주는 것이다. 식은 다음과같다
![logd](https://user-images.githubusercontent.com/68374734/107595939-8db0de80-6c59-11eb-9225-c7ea940afdf2.PNG)
###### 

#### Nesterove Momentum update

###### Nesterove는 전의 Momentum과 비슷하지만 차이가있다. 
###### Momentum 방식에서는 이동 벡터 vt 를 계산할 때 현재 위치에서의 gradient와 momentum step을 독립적으로 계산하고 합친다. 
###### 반면, NAG에서는 momentum step을 먼저 고려하여, momentum step을 먼저 이동했다고 생각한 후 그 자리에서의 gradient를 구해서 gradient step을 이동한다.
###### 다음 사진과 같다.

![logo](https://user-images.githubusercontent.com/68374734/107595938-8d184800-6c59-11eb-922c-c22727bce550.PNG)

#### AdaGrad update

![logo](https://user-images.githubusercontent.com/68374734/107595933-8be71b00-6c59-11eb-8c8b-bb50212c413a.PNG)

#### RMSProp update

![logo](https://user-images.githubusercontent.com/68374734/107595930-8ab5ee00-6c59-11eb-9ace-bf1d7ec7bc13.PNG)

#### Adam update

![logo](https://user-images.githubusercontent.com/68374734/107595935-8c7fb180-6c59-11eb-9e73-a6c55cbc0f8f.PNG)

### Ensemble

### dropout
