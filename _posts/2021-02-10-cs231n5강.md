# cs231n5ê°•

1. activation function 
    - sigmoid
    - tanH
    - ReLU
    - Leaky ReLU
    - PReLU
    - Maxout
2. Data processing

3. Weight Initialization
    - small random number
    - Xavier initialization
    - batch Normalization
    
4. Babyssitting the Learning process

5. Hyper parameter Optimization

### ACtivation funtion 
#### sigmoid  
![logo]
###### 
#### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
