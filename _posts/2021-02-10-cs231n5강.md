# cs231n5강

1. activation function 
    - sigmoid
    - tanH
    - ReLU
    - Leaky ReLU
    - PReLU
    - Maxout
2. Data processing

3. Weight Initialization
    - small random number
    - Xavier initialization
    - batch Normalization
    
4. Babyssitting the Learning process

5. Hyper parameter Optimization

### Activation funtion 
###### 먼저 Activation funtion은 입력값에 대한 출력값이 non-linear하게 되도록 하는 함수이다/
#### sigmoid  
![logo](https://user-images.githubusercontent.com/68374734/107511737-396e1600-6be9-11eb-8082-ad8e9d87c7b4.PNG)
###### sigmoid함수는 다음과 같이 생겼다. 이 함수는 지금 안쓰이는 함수인데 이유는 다음 문제와같다  
###### 1. Gradient Vanishing 문제
######      다음 함수에서 만약 x값이 10 또는 -10 이 넘는 기울기가 급격히 감소되어있는 상태이고 거의 0이라 볼 수 있다. 그렇기 때문에 Gradient가 사라지는 현상이생긴다.
###### 2. Not zero-centered
######      Not zero-centered가 문제인 이유는 현제 그래프로 보면 모든 입력값이 양수이고,w 의 gradient를 보면 dL/dw = dL/da*da/dw
######      요기서 L은 loss a는 W^T*x+b를 의미한다. 그럼 da/dw = x이고 dL/dw = dL/da *x가 된다. 파라미터의 gradient는 입력값에 의해 영향을 받으며
######      입력값이 모두 양수라면 파라미터의 부호는 모두 같게 된다. 그러면 gradient descent 할 때 정확한 방향으로 가지못하고 지그재그로 수렴하는 문제가발생한다
###### 3. exp fn
######      exp함수가 비싼 함수이기 때문에 효율이 떨어진다

#### tanh
![logo](https://user-images.githubusercontent.com/68374734/107511748-3d019d00-6be9-11eb-85dd-52acadd1e3a4.PNG)
###### tanh험슈눈 다음과 같이 생겼다. 이 함수는 sigmoid와는 달리 zero-centered이지만 sigmoid와 마찬가지로
###### gradient vanishing 현상이 일어난다.
#### ReLU
![logo](https://user-images.githubusercontent.com/68374734/107511754-3f63f700-6be9-11eb-8157-6ce7d741714a.PNG)
###### ReLU는 다음과 같이 생겼다. ReLU에서의 문제점은 sigmoid와같이 Not zero-centered이다
###### 그리고 x값이 0이면 gradient는 undefined되고 x값이 0보다 작으면 함수값이 0으로 계속 출력이 되기 때문에 gradient가 죽는다 
#### leakyReLU
![logo](https://user-images.githubusercontent.com/68374734/107511759-41c65100-6be9-11eb-833b-f7d7b004e911.PNG)
###### leakyReLU는 다음과 같이 생겼다. ReLU에서 x<0인 경우 gradient가 죽는 경우를 보완하려고 0.01x로 바꾸었다. 이렇게 되어
###### x<0에서도 죽지 않게 되었다
#### PReLU
![logo](https://user-images.githubusercontent.com/68374734/107511764-43901480-6be9-11eb-8972-554604857dd5.PNG)
###### PReLU는 다음과 같이 x<0일 때 a라는 파라미터를 부여하였다. 그래서 x<0일때 기울기를 트레이닝시킨다
#### ELU
![logo](https://user-images.githubusercontent.com/68374734/107511776-4854c880-6be9-11eb-9ce9-e62bdba988c3.PNG)
###### ELU는 x<0일때 exp함수를 사용한다. 아까도 언급했듯이 exp는 비싼함수라 효율이떨어진다.
#### Maxout 
![logo](https://user-images.githubusercontent.com/68374734/107511770-45f26e80-6be9-11eb-9663-9d050f813e7d.PNG)
###### maxout은 보시다시피 weight를 2개를 사용한다 이것은 Relu 와 Leaky ReLU를 일반화 한것인데 weight가 2개여서 연산도 2배이상 증가게된다.
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
