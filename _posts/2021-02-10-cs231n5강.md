# cs231n5강

1. activation function 
    - sigmoid
    - tanH
    - ReLU
    - Leaky ReLU
    - PReLU
    - Maxout
2. Data processing

3. Weight Initialization
    - small random number
    - Xavier initialization
    - batch Normalization
    
4. Babyssitting the Learning process

5. Hyper parameter Optimization

### Activation funtion 
###### 먼저 Activation funtion은 입력값에 대한 출력값이 non-linear하게 되도록 하는 함수이다/
#### sigmoid  
![logo](https://user-images.githubusercontent.com/68374734/107511737-396e1600-6be9-11eb-8082-ad8e9d87c7b4.PNG)
###### sigmoid함수는 다음과 같이 생겼다. 이 함수는 지금 안쓰이는 함수인데 이유는 다음 문제와같다  
###### 1. Gradient Vanishing 문제
######      다음 함수에서 만약 x값이 10 또는 -10 이 넘는 기울기가 급격히 감소되어있는 상태이고 거의 0이라 볼 수 있다. 그렇기 때문에 Gradient가 사라지는 현상이생긴다.
###### 2. Not zero-centered
######      Not zero-centered가 문제인 이유는 현제 그래프로 보면 모든 입력값이 양수이고,w 의 gradient를 보면 dL/dw = dL/da*da/dw
######      요기서 L은 loss a는 W^T*x+b를 의미한다. 그럼 da/dw = x이고 dL/dw = dL/da *x가 된다. 파라미터의 gradient는 입력값에 의해 영향을 받으며
######      입력값이 모두 양수라면 파라미터의 부호는 모두 같게 된다. 그러면 gradient descent 할 때 정확한 방향으로 가지못하고 지그재그로 수렴하는 문제가발생한다
###### 3. exp fn
######      exp함수가 비싼 함수이기 때문에 효율이 떨어진다

#### tanh
![logo](https://user-images.githubusercontent.com/68374734/107511748-3d019d00-6be9-11eb-85dd-52acadd1e3a4.PNG)
###### tanh험슈눈 다음과 같이 생겼다. 이 함수는 sigmoid와는 달리 zero-centered이지만 sigmoid와 마찬가지로
###### gradient vanishing 현상이 일어난다.
#### ReLU
![logo](https://user-images.githubusercontent.com/68374734/107511754-3f63f700-6be9-11eb-8157-6ce7d741714a.PNG)
###### ReLU는 다음과 같이 생겼다. ReLU에서의 문제점은 sigmoid와같이 Not zero-centered이다
###### 그리고 x값이 0이면 gradient는 undefined되고 x값이 0보다 작으면 함수값이 0으로 계속 출력이 되기 때문에 gradient가 죽는다 
#### leakyReLU
![logo](https://user-images.githubusercontent.com/68374734/107511759-41c65100-6be9-11eb-833b-f7d7b004e911.PNG)
###### leakyReLU는 다음과 같이 생겼다. ReLU에서 x<0인 경우 gradient가 죽는 경우를 보완하려고 0.01x로 바꾸었다. 이렇게 되어
###### x<0에서도 죽지 않게 되었다
#### PReLU
![logo](https://user-images.githubusercontent.com/68374734/107511764-43901480-6be9-11eb-8972-554604857dd5.PNG)
###### PReLU는 다음과 같이 x<0일 때 a라는 파라미터를 부여하였다. 그래서 x<0일때 기울기를 트레이닝시킨다
#### ELU
![logo](https://user-images.githubusercontent.com/68374734/107511776-4854c880-6be9-11eb-9ce9-e62bdba988c3.PNG)
###### ELU는 x<0일때 exp함수를 사용한다. 아까도 언급했듯이 exp는 비싼함수라 효율이떨어진다.
#### Maxout 
![logo](https://user-images.githubusercontent.com/68374734/107511770-45f26e80-6be9-11eb-9663-9d050f813e7d.PNG)
###### maxout은 보시다시피 weight를 2개를 사용한다 이것은 Relu 와 Leaky ReLU를 일반화 한것인데 weight가 2개여서 연산도 2배이상 증가게된다.

### Data Processing 
![logo](https://user-images.githubusercontent.com/68374734/107515364-43464800-6bee-11eb-888c-2a01544524d5.PNG)
###### Data processing 에서는 먼저 original data를 각 data값을 mean값으로 빼서 zero-centered한다. 그리고 그것을 nomalized 시킨다.

### Weight Initialization  
###### 딥러닝 학습에 있어 초기 가중치 설정은 매우 중요한 역활을 한다. 가중치를 잘못 설정할 경우 기울기 소실 문제나 표현력의 한계를 갖는 등 여러 문제를 야기하게 된다. 또한 딥러닝의 학습의 문제가 non-convex 이기 때문에 초기값을 잘못 설정할 경우 local minimum에 수렴할 가능성이 커지게 된다.  
#### small random number 
###### 가중치 초기값은 작은 값으로 초기화 해야하는 데, 그 이유는 활성화 함수가 sigmoid일 경우 만약 가중치 초기값을 큰 값으로 한다면 0과 1로 수렴하기 때문에 그래디언트 소실이 발생
###### 하게 된다. 또한 활성화 함수가 ReLU일 경우 절대값이 클 경우 음수일 때는 dead ReLU 문제가 발생하고, 양수일 때는 그래디언트 폭주가 일어나게 된다. 
###### 따라서, 가중치 초기값을 작게 초기화 해야하며 동일한 초기값을 가지지 않도록 랜덤하게 초기화 해야한다. 일반적으로 가중치 초기값은 평균이 0이고 표준편차가 0.01인 정규분포
###### 를 따르는 값으로 랜덤하게 초기화 한다
###### 이러한 가중치 초기화 방법은 얕은 신경망에서는 괜찮게 작동할지 모르지만 신경망의 깊이가 깊어질수록 문제가 발생하게 된다
###### 그 이유는 깊어지면 깊어질수록 값이 작아지기 때문에 값이 동일하게 발생되어 gradient가 소실되 학습이 이루어지지 않게되기 때문입니다.

#### Xavier initialization
![logo](https://user-images.githubusercontent.com/68374734/107516361-a684aa00-6bef-11eb-8545-4e78913da05d.PNG)
###### 이 Xavier initialization의 코드는 다음과같다. 초기화방법은 이전노드와 다음 노드의 개수에 의존하는 방법이다.
###### 이 함수는 sigmoid와,tanh에서 효과적이 결과를 보여준다. 하지만 ReLU함수에서 사용시 출력값이 0으로 수렴하게 되는 현상이 일어난다.

#### He initialization
![logo](https://user-images.githubusercontent.com/68374734/107511780-4ab72280-6be9-11eb-88c3-8b1170328d9d.PNG)
###### He initialization은 Kaiming He가 만들었고 이 것은 다음 사진과같이 Xavier함수에서 나누기 2를 추가했는데 아까 Xavier initialization에서
###### 발생한 ReLU에서 안되는 현상을 해결하였다.

#### Batch Normalization
###### Batch normalization은 학습의 효율을 높이기 위해 도입되었다.
###### Batch normalization
###### Batch는 신경망의 각 layer에서 데이터의 분포(batch)를 정규화하는 작업이다. 이렇게 배치마다 정규화를 함으로써 전체 데이터에 대한 평균의 분산과 값이 달라질 수 있다.
###### 학습을 할 때마다 활성화값,출력값을 정규화 하기 때문에 가중치 초기화문제에서 자유로워진다
