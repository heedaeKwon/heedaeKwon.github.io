# cs231n5강

1. activation function 
    - sigmoid
    - tanH
    - ReLU
    - Leaky ReLU
    - PReLU
    - Maxout
2. Data processing

3. Weight Initialization
    - small random number
    - Xavier initialization
    - batch Normalization
    
4. Babyssitting the Learning process

5. Hyper parameter Optimization

### ACtivation funtion 
###### 먼저 Activation funtion은 입력값에 대한 출력값이 non-linear하게 되도록 하는 함수이다/
#### sigmoid  
![logo](https://user-images.githubusercontent.com/68374734/107511737-396e1600-6be9-11eb-8082-ad8e9d87c7b4.PNG)
###### sigmoid함수는 다음과 같이 생겼다. 이 함수는 지금 안쓰이는 함수인데 이유는 다음 문제와같다  
###### 1. Gradient Vanishing 문제
###### 2. Not zero-centered
###### 3. exp fn
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
###### 
