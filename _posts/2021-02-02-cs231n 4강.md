### cs231n 4강

###### cs231 4강 내용입니다.
###### 4강은 backpropagation과 Nueral Network입니다.
###### 먼저 backpropagation은 input의 gradient를 구하기 위한
###### 과정이라고 볼 수 있습니다.
###### 구하는 과정은 다음 사진을 예를 들면서 설명하겠습니다
![logo](https://user-images.githubusercontent.com/68374734/106742842-e7654780-6660-11eb-8019-95e3fedd2e99.png)
###### setting  x = -2 , y = 5 , z = -4
###### 요기서 로스 값은 -12가 나왔습니다.
###### 저희가 구하고자하는 것은 input에 대한 gradient 입니다.
###### 그래서 BP(backword pass)를 하려고 합니다. 참고로 f를 구하는 과정은 FP(Foward pass)입니다
###### 이 과정에서 편미분을 이용할 것인데 f에서 x또는 y까지 바로 구할수가 없습니다.
###### 그래서 chain rule 이라는 방식을이용해 q를 생성합니다.
###### 그렇게되면 q = x+y여서 x,y에 편미분한 값들은 dq/dx = 1 ,dq/dy = 1이 나옵니다. (이 값들은 FP과정에서 먼저구합니다.)
###### 자 BP의 시작은 iIdentifu fn 을 구합니다. 이값은 df/df이니 1이 나옵니다
###### 다음은 df/dz , df/dq입니다. f = q*z이니 df/dz = q(x+y=q =3) , df/dq = z(input값 -4)값이 나옵니다
###### 이렇게 z의 gradient는 3의 값이 나옵니다
###### 다음은 df/dx입니다 아까 말했듯이 f를x로 바로 편미분을 못하기때문에 q를 이용합니다 df/dx = df/dq*dq/dx
###### 마찬가지로 df/dy = df/dq*dq/dy (dq/dy는 jacobian matrix이고 df/dq는 vector입니다.) jacobian matrix 는 도함수 행렬입니다
###### dq/dy, dq/dx는 1이고 df/dq =z 즉 z의 input값인 -4이므로
###### x,y의 gradient 는 -4가 나오게됩니다.

###### 다음은 Neural Network입니다
###### 기본적인 형태는 f = Wx입니다. 이 것은 linear한 형태이고
###### 요기서 층을 이제 추가를 하게되면은
###### f = W2max(0,W1x)
###### f = W3max(0,W2max(0,W1x))이런식으로 형태가되는데 이 것은 non-linear한 형태입니다
###### 이렇게  non-linear한 형태를 이용하는이유는
###### linear하게되면은 그래프로 따지면 매우 단순한 형태가 됩니다. 마치 y=x이런 형태처럼 말입니다.
###### 그렇게 되면 많은 데이터들을 이분법적으로 구분해서 정확한 결과값들이 안나올 확률이 매우 높습니다.
###### 그렇기에 보다 복잡한 그래프가 필요로하게 되고 그 형태가 바로 non-linear한 형태입니다.
###### activation Funtions 에는 
###### Sigmoid 1/(1+e^(-x))
###### tanh
###### Relu max(0,x)가 있습니다
###### 이제 결과값들을 정교하게 맞추기위해서는 히든레이어층을 높이면 높일수록 확률이 올라갑니다
![logo](https://user-images.githubusercontent.com/68374734/106742835-e59b8400-6660-11eb-9c33-19c1173fc1ad.png)
###### Regularization과정에서는 람다값이 커야 overfitting이 안이루어져 낮은 값보다 보다 좋은 결과값을 도춣할 수 있습니다
![logo](https://user-images.githubusercontent.com/68374734/106742849-e8967480-6660-11eb-9b37-b11ec46fab1f.png)
###### 사진은 cs231n 2016 lecture4를 이용했습니다.
